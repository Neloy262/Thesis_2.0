{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from threading import Thread,Semaphore\nimport numpy as np\nimport random\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nimport tensorflow_probability as tfp\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom collections import deque","metadata":{"id":"gRRgpXcuex2i","execution":{"iopub.status.busy":"2021-12-31T10:02:54.650871Z","iopub.execute_input":"2021-12-31T10:02:54.651162Z","iopub.status.idle":"2021-12-31T10:02:56.308712Z","shell.execute_reply.started":"2021-12-31T10:02:54.651131Z","shell.execute_reply":"2021-12-31T10:02:56.307770Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"Lambda = 1\nglobal SZ,CP,RM\nscale = 1000000\nSZ=[]\nCP=[]\nRM=[]\n\nminibatch = 32\nk=10000\nC=5\nstate=[]\n#Lambda = 10\n#mu = 1.5\nalpha = 0.3\ngamma = 0.95\n#t = 0   #system clock\nt1=0\nt2=0\nt_lambda = 0 #poisson random variable used to generate next service time\nt_mu = 0  #poisson random variable used to generate next departure time\nt_a=0\nwait=0\nR=100\n\ntransmission_time=0\nl=\"0\"\nP_transmission=500\nP_idle=100\n\nepisodes = 10\nSHOW_EVERY = 100\nSTATS_EVERY=100\n\nQ_dict=dict()\nL=[]\n\naction_list=()\n\ntrain_data=[]\ntrain_label=[]","metadata":{"id":"jby9Jqb3ex2m","execution":{"iopub.status.busy":"2021-12-31T10:02:56.310285Z","iopub.execute_input":"2021-12-31T10:02:56.310510Z","iopub.status.idle":"2021-12-31T10:02:56.321842Z","shell.execute_reply.started":"2021-12-31T10:02:56.310483Z","shell.execute_reply":"2021-12-31T10:02:56.320815Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def reward_generator( T, cpu, bw, m):\n    cpuU=cpu*0.5\n    mU=m*0.5\n    bwU=bw*0.3\n\n    cpusla=680\n    bwsla=301\n    msla=1000\n    Tsla1=1.5\n    Tsla2=3\n\n    Ct1=0\n    Ct2=0\n    Cp=0\n    Cbw=0\n    Cm=0\n    rt=0\n    ru=0\n    \n\n    \n    #if (G<Gsla):\n        #Cg=-5*(Gsla/G)\n    if (cpuU<cpusla):\n        Cp=-4* (cpusla/cpuU)\n    if (mU<msla):\n        Cm=-4* (msla/mU)\n    if (bwU<bwsla):\n        Cbw=-4* (bwsla/bwU)\n    if (T>Tsla1 and T<Tsla2):\n        Ct1 = -5 * (T/Tsla1)\n        rt=(Tsla1/T) + Ct1\n    elif (T>=Tsla2):\n        Ct2 = -6 * (T/Tsla1)\n        rt=(Tsla1/T) + Ct2\n    else:\n        rt=(Tsla1/T)\n        \n\n    #if (N>Nsla):\n        #Cn=-5*(N/Nsla)\n    #if (cost>Cost_sla):\n        #Cost_n=-5*(cost/Cost_sla)\n    ru=(cpuU/cpusla)+Cp+(bwU/bwsla)+Cbw+(mU/msla)+Cm\n    Reward = rt + ru   #Reward Function\n    return Reward","metadata":{"id":"1x4utniRex2n","execution":{"iopub.status.busy":"2021-12-31T10:02:56.323322Z","iopub.execute_input":"2021-12-31T10:02:56.323802Z","iopub.status.idle":"2021-12-31T10:02:56.342000Z","shell.execute_reply.started":"2021-12-31T10:02:56.323757Z","shell.execute_reply":"2021-12-31T10:02:56.341227Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Packet(object):\n    def __init__(self, mobileID):\n                                                        \n        self.mobileID=mobileID   \n        self.size=random.randint(200,400) #seed 1 op 2, seed 2 op \n        SZ.append(round(self.size,2))\n        self.cpucycle=random.uniform(100,1900)\n        self.memory_need=random.uniform(1000,2000)\n        self.ram=random.randint(1, 32) #seed 1 op 3, seed 2 op 1, seed 3 op 4\n        RM.append(round(self.ram,2))\n        \n        \nclass Mobile():\n    \n    def __init__(self,id):\n        \n        self.id = id\n        \n    def packet_generator(self):\n        \n        p = Packet(self.id)\n        \n        return p","metadata":{"id":"_9i8lcNnex2o","execution":{"iopub.status.busy":"2021-12-31T10:02:56.343893Z","iopub.execute_input":"2021-12-31T10:02:56.344300Z","iopub.status.idle":"2021-12-31T10:02:56.353569Z","shell.execute_reply.started":"2021-12-31T10:02:56.344250Z","shell.execute_reply":"2021-12-31T10:02:56.352593Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Q_Learning():\n    \n    def __init__(self,gamma,alpha):\n        \n        self.d = dict()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.epsilon = 1\n        self.epsilon_min = 0.1\n        \n    def calculate_Q_value(self,a,prevState,newState,reward):\n        \n        self.d[(prevState,a)] = self.d.get((prevState,a),0) + self.alpha*(reward + self.gamma * max(self.d.get((newState,0),0),self.d.get((newState,1),0),self.d.get((newState,2),0)) - self.d.get((prevState,action),0))\n        \n    def get_action(self,state):\n        \n        return np.argmax([self.d.get((state,0),0),self.d.get((state,1),0),self.d.get((state,2),0)])\n        \n        ","metadata":{"id":"wViYjHouex2p","execution":{"iopub.status.busy":"2021-12-31T10:02:56.354799Z","iopub.execute_input":"2021-12-31T10:02:56.355221Z","iopub.status.idle":"2021-12-31T10:02:56.365256Z","shell.execute_reply.started":"2021-12-31T10:02:56.355171Z","shell.execute_reply":"2021-12-31T10:02:56.364604Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self,state_size,action_size):\n        self.state_size=state_size\n        self.action_size=action_size\n        self.memory=deque(maxlen=20000)\n        self.gamma=0.99\n        self.epsilon=1.0\n        self.epsilon_decay=0.995\n        self.epsilon_min=0.01\n        self.learning_rate=0.000025\n        self.model=self.build_model()\n        self.target_model=self.build_model()\n        self.target_model.set_weights(self.model.get_weights())\n        self.target_update_counter=0\n        \n    def build_model(self):\n        model=Sequential()\n        model.add(Dense(8,input_dim=self.state_size,activation='relu'))\n        model.add(Dense(8,activation='relu'))\n        model.add(Dense(self.action_size,activation='linear'))\n        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n        return model\n\n    \n    def remember(self,state,action,reward,next_state,done):\n        self.memory.append((state,action,reward,next_state,done))\n    \n    def act(self,state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        state=np.expand_dims(state, axis=0)\n        \n        act_values=self.model.predict(state)\n        return np.argmax(act_values[0])\n        \n        \n    def replay(self,batch_size):\n\n        minibatch=random.sample(self.memory,batch_size)\n        for state,action,reward,next_state,done in minibatch:\n            target=reward\n            state=np.expand_dims(state, axis=0)\n            next_state=np.expand_dims(next_state, axis=0)\n            if not done:\n                target=reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n            target_f=self.model.predict(state)\n            target_f[0][action]=target\n            self.model.fit(state,target_f,epochs=1,verbose=0)\n            \n        if self.epsilon>self.epsilon_min:\n            self.epsilon*=self.epsilon_decay\n    \n    def update_target_network(self):\n        self.target_model.set_weights(self.model.get_weights())\n    \n    def load(self,name):\n        self.model.load_weights(name)\n        \n    def save(self,name):\n        self.model.save_weights(name)\n","metadata":{"id":"pSR1AAZ1gMOM","execution":{"iopub.status.busy":"2021-12-31T10:02:56.366827Z","iopub.execute_input":"2021-12-31T10:02:56.367391Z","iopub.status.idle":"2021-12-31T10:02:56.386877Z","shell.execute_reply.started":"2021-12-31T10:02:56.367341Z","shell.execute_reply":"2021-12-31T10:02:56.385788Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class ActorCriticNetwork(keras.Model):\n    def __init__(self,n_actions,fc1_dims=32,fc2_dims=32):\n        super(ActorCriticNetwork,self).__init__()\n        self.fc1_dims=fc1_dims\n        self.fc2_dims=fc2_dims\n        self.n_actions=n_actions\n        \n        self.fc1=Dense(self.fc1_dims,activation='relu')\n        self.fc2=Dense(self.fc2_dims,activation='relu')\n        self.v=Dense(1,activation='linear')\n        self.pi=Dense(n_actions,activation='softmax')\n        \n    def call(self,state):\n        value=self.fc1(state)\n        value=self.fc2(value)\n        \n        v=self.v(value)\n        pi=self.pi(value)\n        \n        return v,pi\n\nclass Agent:\n    def __init__(self,alpha=0.0003,gamma=0.99,n_actions=2):\n        self.gamma=gamma\n        self.n_actions=n_actions\n        self.action=None\n        self.action_space=[i for i in range(self.n_actions)]\n        \n        self.actor_critic=ActorCriticNetwork(n_actions=n_actions)\n        self.actor_critic.compile(optimizer=Adam(learning_rate=alpha))\n        \n    def choose_action(self,observation):\n        state=tf.convert_to_tensor([observation])\n        _,probs=self.actor_critic(state)\n        \n        action_probabilities=tfp.distributions.Categorical(probs=probs)\n        action=action_probabilities.sample()\n        self.action=action\n        return action.numpy()[0]\n    \n    def learn(self,state,reward,state_,done):\n        state=tf.convert_to_tensor([state],dtype=tf.float32)\n        state_=tf.convert_to_tensor([state_],dtype=tf.float32)\n        reward=tf.convert_to_tensor(reward,dtype=tf.float32)\n        \n        with tf.GradientTape(persistent=True) as tape:\n            state_value,probs=self.actor_critic(state)\n            state_value_,_=self.actor_critic(state_)\n            state_value=tf.squeeze(state_value)\n            state_value_=tf.squeeze(state_value_)\n            \n            action_probs=tfp.distributions.Categorical(probs=probs)\n            log_prob=action_probs.log_prob(self.action)\n            \n            delta=reward+self.gamma*state_value_*(1-int(done))-state_value\n            actor_loss=-log_prob*delta\n            critic_loss=delta**2\n            \n            total_loss=actor_loss+critic_loss\n            \n        gradient=tape.gradient(total_loss,self.actor_critic.trainable_variables)\n        self.actor_critic.optimizer.apply_gradients(zip(gradient,self.actor_critic.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T10:02:56.388334Z","iopub.execute_input":"2021-12-31T10:02:56.388839Z","iopub.status.idle":"2021-12-31T10:02:56.409691Z","shell.execute_reply.started":"2021-12-31T10:02:56.388790Z","shell.execute_reply":"2021-12-31T10:02:56.408795Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class Fog:\n    \n    def __init__(self,num_server,mu,k,ID):\n        \n        self.num_server = num_server\n        self.mu = mu\n        self.lock = Semaphore(self.num_server)\n        self.Serverqueue = 0\n        self.waitTime = []\n        self.responseTime = []\n        self.reward_list = []\n        self.cpuspeed=0\n        self.cpucycle=0\n        self.memory=0\n        self.R=0\n        self.ID = ID\n        self.neighbour_list = []\n        \n        self.lost = 0\n        self.transmission_time = 0\n        \n    def run(self, packet, State):\n        self.R=random.uniform(900,1100)\n        self.transmission_time = (packet.size/self.R)\n        \n        time.sleep(((-np.log(random.random())/Lambda)+self.transmission_time)/scale)\n\n        t = Thread(target=self.worker,args=(time.time(),packet,State))\n        \n        self.Serverqueue+=1\n        t.start()\n        \n        \n        \n    def getState(self):\n        \n        return (self.Serverqueue, self.cpuspeed, self.R, self.memory)\n    \n    def set_neighbour(self,n1,n2):\n        self.neighbour_list.append(n1)\n        self.neighbour_list.append(n2)\n    \n    \n    def worker(self,arrival,packet,State):\n        \n        r = 0\n        \n        new_cpuspeed = 0\n        new_mem = 0\n        new_queuelength = 0\n        \n        if self.lock.acquire():\n            \n            self.cpuspeed=random.uniform(1000,1500)\n            new_cpuspeed = self.cpuspeed\n            \n            \n            self.memory=random.uniform(3000,4000)\n            self.memory=self.memory-packet.memory_need \n            new_mem = self.memory\n            \n            \n            self.waitTime.append((time.time()-arrival)*scale)\n            \n            t_mu=packet.cpucycle/self.cpuspeed\n            \n            time.sleep(t_mu/scale)\n            \n            response_time = time.time() - arrival\n            \n            self.responseTime.append(response_time*scale)\n            \n            r=reward_generator(response_time,self.cpuspeed,self.R,self.memory)\n            self.reward_list.append(r)\n            \n            self.Serverqueue-=1\n            new_queuelength = self.Serverqueue\n            \n            \n \n        self.lock.release()\n        new_state = (new_queuelength,new_cpuspeed,self.R,new_mem) + self.neighbour_list[0].getState() + self.neighbour_list[1].getState()\n        AC.learn(State,r,new_state,False)\n        #deep_Q.remember(State,self.ID,r,new_state,False)\n        # Qlearning.calculate_Q_value(self.ID,State,new_state,r)","metadata":{"id":"O2BQsCGIex2q","execution":{"iopub.status.busy":"2021-12-31T10:02:56.411296Z","iopub.execute_input":"2021-12-31T10:02:56.411629Z","iopub.status.idle":"2021-12-31T10:02:56.432883Z","shell.execute_reply.started":"2021-12-31T10:02:56.411584Z","shell.execute_reply":"2021-12-31T10:02:56.432001Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"c1 = Fog(3,1.25,10,0)\nc2 = Fog(3,1.25,10,1)\nc3 = Fog(3,1.25,10,2)\n\nc1.set_neighbour(c2,c3)\nc2.set_neighbour(c1,c3)\nc3.set_neighbour(c1,c2)\n\nm1 = Mobile(1)","metadata":{"id":"iTNjbNZTex2r","execution":{"iopub.status.busy":"2021-12-31T10:02:56.434464Z","iopub.execute_input":"2021-12-31T10:02:56.435232Z","iopub.status.idle":"2021-12-31T10:02:56.449274Z","shell.execute_reply.started":"2021-12-31T10:02:56.435180Z","shell.execute_reply":"2021-12-31T10:02:56.447960Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"startSim = time.time()\n# Qlearning = Q_Learning(0.95,0.1)\ndeep_Q = DQNAgent(12,3)\nAC = Agent(alpha=1e-5,gamma=0.95,n_actions=3)\nC = 0\nN=10000\n# state 1\nfor i in range(N):\n    print(\"Iteration:\",i)\n    p1 = m1.packet_generator()\n#     p2 = m2.packet_generator()\n#     p3 = m3.packet_generator()\n    \n    State = c1.getState() + c2.getState() + c3.getState() \n    \n    action = AC.choose_action(State)\n    \n#     if i<=20000:\n#         action = random.randint(0,2)\n#     else:\n        #action = deep_Q.act(State)\n        \n        # action = np.random.choice([action,random.randint(0,2)], p=[1 - Qlearning.epsilon , Qlearning.epsilon])\n    \n    if action == 0:\n        c1.run(p1,State)\n    elif action == 1:\n        c2.run(p1,State)\n    elif action == 2:\n        c2.run(p1,State)\n\n#     if len(deep_Q.memory) >= minibatch :\n#       deep_Q.replay(minibatch)\n    \n#     if C==32:\n#       deep_Q.update_target_network()\n#       C=0\n\n#     C+=1\n\n    # if Qlearning.epsilon>Qlearning.epsilon_min:\n    #     Qlearning.epsilon = Qlearning.epsilon - (Qlearning.epsilon*0.01)\n    # else:\n    #     Qlearning.epsilon = 0.1\n    \nprint(\"Simulation time:\",time.time()-startSim)","metadata":{"id":"kOovkJP5ex2s","outputId":"6ae902b6-2665-4288-f9eb-6a6079d39198","execution":{"iopub.status.busy":"2021-12-31T10:02:57.786739Z","iopub.execute_input":"2021-12-31T10:02:57.787355Z","iopub.status.idle":"2021-12-31T10:03:36.968110Z","shell.execute_reply.started":"2021-12-31T10:02:57.787302Z","shell.execute_reply":"2021-12-31T10:03:36.966917Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"acluPnW8ex2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Jltp7Flrex2t"},"execution_count":null,"outputs":[]}]}